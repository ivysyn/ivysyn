--- common_utils.py	2022-06-30 17:09:46.850681367 -0400
+++ /home/ivyusr/common_utils.py	2022-06-30 17:09:42.638642148 -0400
@@ -71,29 +71,39 @@ from torch.testing._internal.common_dtyp
 from torch.nn import ModuleList, ModuleDict, Sequential, ParameterList, ParameterDict
 from torch._C import ScriptList, ScriptDict  # type: ignore[attr-defined]
 
+import signal
+import glob
+# import pdb
+
+MAX_TIMEOUT_NS = 1500 * 1e+9
+
 torch.backends.disable_global_flags()
 
+RESULTS_PATH = "/mnt/pytorch-ivysyn/"
+TESTS_DURATIONS_PATH = RESULTS_PATH + "test_durations.txt"
+
 FILE_SCHEMA = "file://"
-if sys.platform == 'win32':
+if sys.platform == "win32":
     FILE_SCHEMA = "file:///"
 
 # Environment variable `IN_CI` is set in `.jenkins/common.sh`.
-IS_IN_CI = os.getenv('IN_CI') == '1'
-IS_SANDCASTLE = os.getenv('SANDCASTLE') == '1' or os.getenv('TW_JOB_USER') == 'sandcastle'
-IS_FBCODE = os.getenv('PYTORCH_TEST_FBCODE') == '1'
-IS_REMOTE_GPU = os.getenv('PYTORCH_TEST_REMOTE_GPU') == '1'
+IS_IN_CI = os.getenv("IN_CI") == "1"
+IS_SANDCASTLE = os.getenv("SANDCASTLE") == "1" or os.getenv("TW_JOB_USER") == "sandcastle"
+IS_FBCODE = os.getenv("PYTORCH_TEST_FBCODE") == "1"
+IS_REMOTE_GPU = os.getenv("PYTORCH_TEST_REMOTE_GPU") == "1"
 
-RETRY_TEST_CASES = os.getenv('PYTORCH_RETRY_TEST_CASES') == '1'
-OVERRIDE_FLAKY_SIGNAL = os.getenv('PYTORCH_OVERRIDE_FLAKY_SIGNAL') == '1'
+RETRY_TEST_CASES = os.getenv("PYTORCH_RETRY_TEST_CASES") == "1"
+OVERRIDE_FLAKY_SIGNAL = os.getenv("PYTORCH_OVERRIDE_FLAKY_SIGNAL") == "1"
 MAX_NUM_RETRIES = 3
 
-DISABLED_TESTS_FILE = '.pytorch-disabled-tests.json'
-SLOW_TESTS_FILE = '.pytorch-slow-tests.json'
+DISABLED_TESTS_FILE = ".pytorch-disabled-tests.json"
+SLOW_TESTS_FILE = ".pytorch-slow-tests.json"
 
 slow_tests_dict: Optional[Dict[str, Any]] = None
 disabled_tests_dict: Optional[Dict[str, Any]] = None
 
-NATIVE_DEVICES = ('cpu', 'cuda', 'meta')
+NATIVE_DEVICES = ("cpu", "cuda", "meta")
+
 
 class _TestParametrizer(object):
     """
@@ -112,6 +122,7 @@ class _TestParametrizer(object):
     composite 'parametrize_fn' will be created that generates tests with the product of the parameters
     generated by the old and new parametrize_fns. This allows for convenient composability of decorators.
     """
+
     def _parametrize_test(self, test, generic_cls, device_cls):
         """
         Parametrizes the given test function across whatever dimension is specified by the derived class.
@@ -134,7 +145,7 @@ class _TestParametrizer(object):
         raise NotImplementedError
 
     def __call__(self, fn):
-        if hasattr(fn, 'parametrize_fn'):
+        if hasattr(fn, "parametrize_fn"):
             # Do composition with the product of args.
             old_parametrize_fn = fn.parametrize_fn
             new_parametrize_fn = self._parametrize_test
@@ -160,24 +171,26 @@ def compose_parametrize_fns(old_parametr
         new_parametrize_fn (callable) - Second parametrize_fn to compose.
     """
 
-    def composite_fn(test, generic_cls, device_cls,
-                     old_parametrize_fn=old_parametrize_fn,
-                     new_parametrize_fn=new_parametrize_fn):
-        old_tests = [(test, test_name, param_kwargs) for (test, test_name, param_kwargs) in
-                     old_parametrize_fn(test, generic_cls, device_cls)]
+    def composite_fn(
+        test, generic_cls, device_cls, old_parametrize_fn=old_parametrize_fn, new_parametrize_fn=new_parametrize_fn
+    ):
+        old_tests = [
+            (test, test_name, param_kwargs)
+            for (test, test_name, param_kwargs) in old_parametrize_fn(test, generic_cls, device_cls)
+        ]
         for (old_test, old_test_name, old_param_kwargs) in old_tests:
-            for (new_test, new_test_name, new_param_kwargs) in \
-                    new_parametrize_fn(old_test, generic_cls, device_cls):
+            for (new_test, new_test_name, new_param_kwargs) in new_parametrize_fn(old_test, generic_cls, device_cls):
                 redundant_params = set(old_param_kwargs.keys()).intersection(new_param_kwargs.keys())
                 if redundant_params:
-                    raise RuntimeError('Parametrization over the same parameter by multiple parametrization '
-                                       'decorators is not supported. For test "{}", the following parameters '
-                                       'are handled multiple times: {}'.format(
-                                           test.__name__, redundant_params))
+                    raise RuntimeError(
+                        "Parametrization over the same parameter by multiple parametrization "
+                        'decorators is not supported. For test "{}", the following parameters '
+                        "are handled multiple times: {}".format(test.__name__, redundant_params)
+                    )
                 full_param_kwargs = {**old_param_kwargs, **new_param_kwargs}
-                merged_test_name = '{}{}{}'.format(new_test_name,
-                                                   '_' if old_test_name != '' and new_test_name != '' else '',
-                                                   old_test_name)
+                merged_test_name = "{}{}{}".format(
+                    new_test_name, "_" if old_test_name != "" and new_test_name != "" else "", old_test_name
+                )
                 yield (new_test, merged_test_name, full_param_kwargs)
 
     return composite_fn
@@ -194,7 +207,7 @@ def instantiate_parametrized_tests(gener
     """
     for attr_name in tuple(dir(generic_cls)):
         class_attr = getattr(generic_cls, attr_name)
-        if not hasattr(class_attr, 'parametrize_fn'):
+        if not hasattr(class_attr, "parametrize_fn"):
             continue
 
         # Remove the generic test from the test class.
@@ -210,8 +223,9 @@ def instantiate_parametrized_tests(gener
             setattr(generic_cls, name, instantiated_test)
 
         for (test, test_suffix, param_kwargs) in class_attr.parametrize_fn(
-                class_attr, generic_cls=generic_cls, device_cls=None):
-            full_name = '{}_{}'.format(test.__name__, test_suffix)
+            class_attr, generic_cls=generic_cls, device_cls=None
+        ):
+            full_name = "{}_{}".format(test.__name__, test_suffix)
             instantiate_test_helper(cls=generic_cls, name=full_name, test=test, param_kwargs=param_kwargs)
 
 
@@ -227,7 +241,8 @@ class subtest(object):
         name (str): Optional name to use for the test.
         decorators (iterable): Iterable of decorators to apply to the generated test.
     """
-    __slots__ = ['arg_values', 'name', 'decorators']
+
+    __slots__ = ["arg_values", "name", "decorators"]
 
     def __init__(self, arg_values, name=None, decorators=None):
         self.arg_values = arg_values
@@ -286,8 +301,9 @@ class parametrize(_TestParametrizer):
             tuples of arg values (e.g. [(1, 2), (3, 4)]).
         name_fn (callable): Optional function that takes in parameters and returns subtest name.
     """
+
     def __init__(self, arg_str, arg_values, name_fn=None):
-        self.arg_names = arg_str.split(',')
+        self.arg_names = arg_str.split(",")
         self.arg_values = arg_values
         self.name_fn = name_fn
 
@@ -298,14 +314,14 @@ class parametrize(_TestParametrizer):
         elif isinstance(value, torch.device):
             return str(value)
         # Can't use isinstance as it would cause a circular import
-        elif value.__class__.__name__ == 'OpInfo' or value.__class__.__name__ == 'ModuleInfo':
+        elif value.__class__.__name__ == "OpInfo" or value.__class__.__name__ == "ModuleInfo":
             return value.formatted_name
         else:
             # Include name and value separated by underscore.
-            return '{}_{}'.format(name, str(value).replace('.', '_'))
+            return "{}_{}".format(name, str(value).replace(".", "_"))
 
     def _default_subtest_name(self, values):
-        return '_'.join([self._formatted_str_repr(a, v) for a, v in zip(self.arg_names, values)])
+        return "_".join([self._formatted_str_repr(a, v) for a, v in zip(self.arg_names, values)])
 
     def _get_subtest_name(self, values, explicit_name=None):
         if explicit_name:
@@ -319,7 +335,7 @@ class parametrize(_TestParametrizer):
     def _parametrize_test(self, test, generic_cls, device_cls):
         if len(self.arg_names) == 0:
             # No additional parameters needed for the test.
-            test_name = ''
+            test_name = ""
             yield (test, test_name, {})
         else:
             # Each "values" item is expected to be either:
@@ -346,17 +362,16 @@ class parametrize(_TestParametrizer):
 
                 values = list(values) if len(self.arg_names) > 1 else [values]
                 if len(values) != len(self.arg_names):
-                    raise RuntimeError('Expected # values == # arg names, but got: {} '
-                                       'values and {} names for test "{}"'.format(
-                                           len(values), len(self.arg_names), test.__name__))
-
-                param_kwargs = {
-                    name: value for name, value in zip(self.arg_names, values)
-                }
+                    raise RuntimeError(
+                        "Expected # values == # arg names, but got: {} "
+                        'values and {} names for test "{}"'.format(len(values), len(self.arg_names), test.__name__)
+                    )
+
+                param_kwargs = {name: value for name, value in zip(self.arg_names, values)}
 
                 test_name = self._get_subtest_name(values, explicit_name=maybe_name)
-                if '.' in test_name:
-                    raise RuntimeError('Test name cannot contain periods, but got: {}'.format(test_name))
+                if "." in test_name:
+                    raise RuntimeError("Test name cannot contain periods, but got: {}".format(test_name))
 
                 yield (gen_test, test_name, param_kwargs)
 
@@ -366,6 +381,7 @@ class ProfilingMode(Enum):
     SIMPLE = 2
     PROFILING = 3
 
+
 def cppProfilingFlagsToProfilingMode():
     old_prof_exec_state = torch._C._jit_set_profiling_executor(True)
     old_prof_mode_state = torch._C._jit_set_profiling_mode(True)
@@ -380,6 +396,7 @@ def cppProfilingFlagsToProfilingMode():
     else:
         return ProfilingMode.LEGACY
 
+
 @contextmanager
 def enable_profiling_mode_for_profiling_tests():
     if GRAPH_EXECUTOR == ProfilingMode.PROFILING:
@@ -392,6 +409,7 @@ def enable_profiling_mode_for_profiling_
             torch._C._jit_set_profiling_executor(old_prof_exec_state)
             torch._C._jit_set_profiling_mode(old_prof_mode_state)
 
+
 @contextmanager
 def enable_profiling_mode():
     old_prof_exec_state = torch._C._jit_set_profiling_executor(True)
@@ -402,6 +420,7 @@ def enable_profiling_mode():
         torch._C._jit_set_profiling_executor(old_prof_exec_state)
         torch._C._jit_set_profiling_mode(old_prof_mode_state)
 
+
 @contextmanager
 def num_profiled_runs(num_runs):
     old_num_runs = torch._C._jit_set_num_profiled_runs(num_runs)
@@ -410,12 +429,14 @@ def num_profiled_runs(num_runs):
     finally:
         torch._C._jit_set_num_profiled_runs(old_num_runs)
 
+
 func_call = torch._C.ScriptFunction.__call__
 meth_call = torch._C.ScriptMethod.__call__
 
+
 def prof_callable(callable, *args, **kwargs):
-    if 'profile_and_replay' in kwargs:
-        del kwargs['profile_and_replay']
+    if "profile_and_replay" in kwargs:
+        del kwargs["profile_and_replay"]
         if GRAPH_EXECUTOR == ProfilingMode.PROFILING:
             with enable_profiling_mode_for_profiling_tests():
                 callable(*args, **kwargs)
@@ -423,57 +444,67 @@ def prof_callable(callable, *args, **kwa
 
     return callable(*args, **kwargs)
 
+
 def prof_func_call(*args, **kwargs):
     return prof_callable(func_call, *args, **kwargs)
 
+
 def prof_meth_call(*args, **kwargs):
     return prof_callable(meth_call, *args, **kwargs)
 
+
 # TODO fix when https://github.com/python/mypy/issues/2427 is address
 torch._C.ScriptFunction.__call__ = prof_func_call  # type: ignore[assignment]
 torch._C.ScriptMethod.__call__ = prof_meth_call  # type: ignore[assignment]
 
+
 def _get_test_report_path():
     # allow users to override the test file location. We need this
     # because the distributed tests run the same test file multiple
     # times with different configurations.
-    override = os.environ.get('TEST_REPORT_SOURCE_OVERRIDE')
-    test_source = override if override is not None else 'python-unittest'
-    return os.path.join('test-reports', test_source)
+    override = os.environ.get("TEST_REPORT_SOURCE_OVERRIDE")
+    test_source = override if override is not None else "python-unittest"
+    return os.path.join("test-reports", test_source)
 
 
 parser = argparse.ArgumentParser()
-parser.add_argument('--subprocess', action='store_true',
-                    help='whether to run each test in a subprocess')
-parser.add_argument('--seed', type=int, default=1234)
-parser.add_argument('--accept', action='store_true')
-parser.add_argument('--jit_executor', type=str)
-parser.add_argument('--repeat', type=int, default=1)
-parser.add_argument('--test_bailouts', action='store_true')
-parser.add_argument('--save-xml', nargs='?', type=str,
-                    const=_get_test_report_path(),
-                    default=_get_test_report_path() if IS_IN_CI else None)
-parser.add_argument('--discover-tests', action='store_true')
-parser.add_argument('--log-suffix', type=str, default="")
-parser.add_argument('--run-parallel', type=int, default=1)
-parser.add_argument('--import-slow-tests', type=str, nargs='?', const=SLOW_TESTS_FILE)
-parser.add_argument('--import-disabled-tests', type=str, nargs='?', const=DISABLED_TESTS_FILE)
+parser.add_argument("--subprocess", action="store_true", help="whether to run each test in a subprocess")
+parser.add_argument("--seed", type=int, default=1234)
+parser.add_argument("--accept", action="store_true")
+parser.add_argument("--jit_executor", type=str)
+parser.add_argument("--repeat", type=int, default=1)
+parser.add_argument("--test_bailouts", action="store_true")
+parser.add_argument(
+    "--save-xml",
+    nargs="?",
+    type=str,
+    const=_get_test_report_path(),
+    default=_get_test_report_path() if IS_IN_CI else None,
+)
+parser.add_argument("--discover-tests", action="store_true")
+parser.add_argument("--log-suffix", type=str, default="")
+parser.add_argument("--run-parallel", type=int, default=1)
+parser.add_argument("--import-slow-tests", type=str, nargs="?", const=SLOW_TESTS_FILE)
+parser.add_argument("--import-disabled-tests", type=str, nargs="?", const=DISABLED_TESTS_FILE)
 
 # Only run when -h or --help flag is active to display both unittest and parser help messages.
+
+
 def run_unittest_help(argv):
     unittest.main(argv=argv)
 
-if '-h' in sys.argv or '--help' in sys.argv:
+
+if "-h" in sys.argv or "--help" in sys.argv:
     help_thread = threading.Thread(target=run_unittest_help, args=(sys.argv,))
     help_thread.start()
     help_thread.join()
 
 args, remaining = parser.parse_known_args()
-if args.jit_executor == 'legacy':
+if args.jit_executor == "legacy":
     GRAPH_EXECUTOR = ProfilingMode.LEGACY
-elif args.jit_executor == 'profiling':
+elif args.jit_executor == "profiling":
     GRAPH_EXECUTOR = ProfilingMode.PROFILING
-elif args.jit_executor == 'simple':
+elif args.jit_executor == "simple":
     GRAPH_EXECUTOR = ProfilingMode.SIMPLE
 else:
     # infer flags based on the default settings
@@ -498,6 +529,7 @@ torch.manual_seed(SEED)
 # CI Prefix path used only on CI environment
 CI_TEST_PREFIX = str(Path(os.getcwd()))
 
+
 def wait_for_process(p):
     try:
         return p.wait()
@@ -517,6 +549,7 @@ def wait_for_process(p):
         # Always call p.wait() to ensure exit
         p.wait()
 
+
 def shell(command, cwd=None, env=None):
     sys.stdout.flush()
     sys.stderr.flush()
@@ -541,8 +574,10 @@ def discover_test_cases_recursively(suit
         rc.extend(discover_test_cases_recursively(element))
     return rc
 
+
 def get_test_names(test_cases):
-    return ['.'.join(case.id().split('.')[-2:]) for case in test_cases]
+    return [".".join(case.id().split(".")[-2:]) for case in test_cases]
+
 
 def _print_test_names():
     suite = unittest.TestLoader().loadTestsFromModule(__main__)
@@ -550,16 +585,21 @@ def _print_test_names():
     for name in get_test_names(test_cases):
         print(name)
 
+
 def chunk_list(lst, nchunks):
     return [lst[i::nchunks] for i in range(nchunks)]
 
+
 # sanitize filename e.g., distributed/pipeline/sync/skip/test_api.py -> distributed.pipeline.sync.skip.test_api
+
+
 def sanitize_test_filename(filename):
     # inspect.getfile returns absolute path in some CI jobs, converting it to relative path if needed
     if filename.startswith(CI_TEST_PREFIX):
         filename = filename[len(CI_TEST_PREFIX) + 1:]
-    strip_py = re.sub(r'.py$', '', filename)
-    return re.sub('/', r'.', strip_py)
+    strip_py = re.sub(r".py$", "", filename)
+    return re.sub("/", r".", strip_py)
+
 
 def lint_test_case_extension(suite):
     succeed = True
@@ -572,29 +612,30 @@ def lint_test_case_extension(suite):
             test_case = first_test
 
         if test_case is not None:
-            test_class = test_case.id().split('.', 1)[1].split('.')[0]
+            test_class = test_case.id().split(".", 1)[1].split(".")[0]
             if not isinstance(test_case, TestCase):
                 err = "This test class should extend from torch.testing._internal.common_utils.TestCase but it doesn't."
                 print(f"{test_class} - failed. {err}")
                 succeed = False
     return succeed
 
+
 def run_tests(argv=UNITTEST_ARGS):
     # import test files.
     if IMPORT_SLOW_TESTS:
         if os.path.exists(IMPORT_SLOW_TESTS):
             global slow_tests_dict
-            with open(IMPORT_SLOW_TESTS, 'r') as fp:
+            with open(IMPORT_SLOW_TESTS, "r") as fp:
                 slow_tests_dict = json.load(fp)
         else:
-            print(f'[WARNING] slow test file provided but not found: {IMPORT_SLOW_TESTS}')
+            print(f"[WARNING] slow test file provided but not found: {IMPORT_SLOW_TESTS}")
     if IMPORT_DISABLED_TESTS:
         if os.path.exists(IMPORT_DISABLED_TESTS):
             global disabled_tests_dict
-            with open(IMPORT_DISABLED_TESTS, 'r') as fp:
+            with open(IMPORT_DISABLED_TESTS, "r") as fp:
                 disabled_tests_dict = json.load(fp)
         else:
-            print(f'[WARNING] disabled test file provided but not found: {IMPORT_DISABLED_TESTS}')
+            print(f"[WARNING] disabled test file provided but not found: {IMPORT_DISABLED_TESTS}")
     # Determine the test launch mechanism
     if TEST_DISCOVER:
         _print_test_names()
@@ -609,18 +650,18 @@ def run_tests(argv=UNITTEST_ARGS):
         failed_tests = []
         test_cases = discover_test_cases_recursively(suite)
         for case in test_cases:
-            test_case_full_name = case.id().split('.', 1)[1]
+            test_case_full_name = case.id().split(".", 1)[1]
             other_args = []
             if IMPORT_DISABLED_TESTS:
-                other_args.append('--import-disabled-tests')
+                other_args.append("--import-disabled-tests")
             if IMPORT_SLOW_TESTS:
-                other_args.append('--import-slow-tests')
+                other_args.append("--import-slow-tests")
             cmd = [sys.executable] + [argv[0]] + other_args + argv[1:] + [test_case_full_name]
             string_cmd = " ".join(cmd)
             exitcode = shell(cmd)
             if exitcode != 0:
                 # This is sort of hacky, but add on relevant env variables for distributed tests.
-                if 'TestDistBackendWithSpawn' in test_case_full_name:
+                if "TestDistBackendWithSpawn" in test_case_full_name:
                     backend = os.environ.get("BACKEND", "")
                     world_size = os.environ.get("WORLD_SIZE", "")
                     env_prefix = f"BACKEND={backend} WORLD_SIZE={world_size}"
@@ -630,29 +671,135 @@ def run_tests(argv=UNITTEST_ARGS):
                 failed_tests.append(test_case_full_name)
 
         assert len(failed_tests) == 0, "{} unit test(s) failed:\n\t{}".format(
-            len(failed_tests), '\n\t'.join(failed_tests))
+            len(failed_tests), "\n\t".join(failed_tests)
+        )
     elif RUN_PARALLEL > 1:
         test_cases = discover_test_cases_recursively(suite)
         test_batches = chunk_list(get_test_names(test_cases), RUN_PARALLEL)
-        processes = []
+
+        testfile_name = argv[0]
+        test_starttime = time.time_ns()
+        processes = {}
         for i in range(RUN_PARALLEL):
-            command = [sys.executable] + argv + ['--log-suffix=-shard-{}'.format(i + 1)] + test_batches[i]
-            processes.append(subprocess.Popen(command, universal_newlines=True))
-        failed = False
-        for p in processes:
-            failed |= wait_for_process(p) != 0
-        assert not failed, "Some test shards have failed"
+            command = [sys.executable] + argv + ["--log-suffix=-shard-{}".format(i + 1)] + test_batches[i]
+            proc = subprocess.Popen(command, universal_newlines=True)
+            # print(f"Running {command}")
+            processes[proc] = None
+        # pdb.set_trace()
+        done_procs = 0
+        while done_procs < RUN_PARALLEL:
+
+            # print(f"Done procs: {done_procs}")
+
+            procs_to_remove = []
+            procs_to_add = []
+
+            for p, proc_kernel_time_tuple in processes.items():
+
+                pid = p.pid
+                # Get time mutations file for the current process
+                kernel_mutfile = glob.glob(RESULTS_PATH + f"*_mutations.log.{pid}")
+
+                if len(kernel_mutfile) > 0:
+                    kernel_mutfile = kernel_mutfile[0]
+                else:
+                    # If it doesn't exit (proc is not fuzzing a kernel),
+                    # check if it finished or timed out and continue
+
+                    if p not in processes or processes[p] is None:
+                        processes[p] = (None, time.time_ns())
+                        proc_kernel = None
+
+                    _, proc_start_time = processes[p]
+                    cur_time = time.time_ns()
+                    elapsed = cur_time - proc_start_time
+                    if elapsed > MAX_TIMEOUT_NS:
+                        print(f"Process timed out (Run for {elapsed // 1e+9}s), killing")
+                        p.kill()
+                        p.wait()
+                        done_procs += 1
+                        procs_to_remove.append(p)
+                        continue
+
+                    retcode = p.poll()
+                    if retcode is not None:
+                        done_procs += 1
+                        p.wait()
+                        procs_to_remove.append(p)
+
+                    continue
+
+                fuzzed_kernel_filename = kernel_mutfile[: kernel_mutfile.index("_mutations")]
+                fuzzed_kernel = fuzzed_kernel_filename.split('/')[-1]
+
+                if proc_kernel_time_tuple is None:
+                    proc_kernel = fuzzed_kernel
+                    print(f"Kernel {proc_kernel} started fuzzing")
+                    processes[p] = (fuzzed_kernel, time.time_ns())
+                elif proc_kernel != fuzzed_kernel:
+                    # We started fuzzing a new kernel
+                    proc_kernel = fuzzed_kernel
+                    # Save the (rough) starting time of this kernel
+                    processes[p] = (fuzzed_kernel, time.time_ns())
+                else:
+                    proc_kernel, kernel_start_time = processes[p]
+                    cur_time = time.time_ns()
+                    elapsed = cur_time - kernel_start_time
+
+                retcode = p.poll()
+                if retcode is not None:
+                    if retcode < 0:
+                        print(f"Process {pid} ({fuzzed_kernel}) exited with code {retcode}")
+                        if retcode == -signal.SIGKILL:
+                            kill_time = int(
+                                time.clock_gettime(time.CLOCK_MONOTONIC))
+                            killed_filename = fuzzed_kernel_filename + ".killed"
+                            with open(killed_filename, "w") as kf:
+                                kf.write(str(kill_time))
+                        procs_to_add.append(p)
+                    else:
+                        print(f"Process {pid} ({fuzzed_kernel}) exited normally")
+                        # Finished without crashing
+                        done_procs += 1
+                    p.wait()
+                    procs_to_remove.append(p)
+
+            for p in procs_to_remove:
+                del processes[p]
+
+            for p in procs_to_add:
+                proc = subprocess.Popen(p.args)
+                processes[proc] = None
+
+            time.sleep(1)
+
+        # print(f"Waiting for {len(processes)} procs to finish...")
+        for p in processes.keys():
+            p.wait()
+
+        elapsed = time.time_ns() - test_starttime
+        elapsed = elapsed / 1e+9
+        with open(TESTS_DURATIONS_PATH, "a") as f:
+            f.write(testfile_name + " " + str(elapsed) + "\n")
+            f.flush()
+        # failed = False
+        # for p in processes:
+        #     failed |= wait_for_process(p) != 0
+        # assert not failed, "Some test shards have failed"
     elif TEST_SAVE_XML is not None:
         # import here so that non-CI doesn't need xmlrunner installed
         import xmlrunner  # type: ignore[import]
+
         test_filename = sanitize_test_filename(inspect.getfile(sys._getframe(1)))
         test_report_path = TEST_SAVE_XML + LOG_SUFFIX
         test_report_path = os.path.join(test_report_path, test_filename)
         os.makedirs(test_report_path, exist_ok=True)
-        verbose = '--verbose' in argv or '-v' in argv
+        verbose = "--verbose" in argv or "-v" in argv
         if verbose:
-            print('Test results will be stored in {}'.format(test_report_path))
-        unittest.main(argv=argv, testRunner=xmlrunner.XMLTestRunner(output=test_report_path, verbosity=2 if verbose else 1))
+            print("Test results will be stored in {}".format(test_report_path))
+        unittest.main(
+            argv=argv, testRunner=xmlrunner.XMLTestRunner(output=test_report_path, verbosity=2 if verbose else 1)
+        )
     elif REPEAT_COUNT > 1:
         for _ in range(REPEAT_COUNT):
             if not unittest.main(exit=False, argv=argv).result.wasSuccessful():
@@ -660,44 +807,53 @@ def run_tests(argv=UNITTEST_ARGS):
     else:
         unittest.main(argv=argv)
 
+
 IS_LINUX = sys.platform == "linux"
 IS_WINDOWS = sys.platform == "win32"
 IS_MACOS = sys.platform == "darwin"
 IS_PPC = platform.machine() == "ppc64le"
 
+
 def is_avx512_vnni_supported():
-    if sys.platform != 'linux':
+    if sys.platform != "linux":
         return False
     with open("/proc/cpuinfo", encoding="ascii") as f:
         lines = f.read()
     return "vnni" in lines
 
+
 IS_AVX512_VNNI_SUPPORTED = is_avx512_vnni_supported()
 
 if IS_WINDOWS:
+
     @contextmanager
     def TemporaryFileName(*args, **kwargs):
         # Ideally we would like to not have to manually delete the file, but NamedTemporaryFile
         # opens the file, and it cannot be opened multiple times in Windows. To support Windows,
         # close the file after creation and try to remove it manually
-        if 'delete' in kwargs:
-            if kwargs['delete'] is not False:
+        if "delete" in kwargs:
+            if kwargs["delete"] is not False:
                 raise UserWarning("only TemporaryFileName with delete=False is supported on Windows.")
         else:
-            kwargs['delete'] = False
+            kwargs["delete"] = False
         f = tempfile.NamedTemporaryFile(*args, **kwargs)
         try:
             f.close()
             yield f.name
         finally:
             os.unlink(f.name)
+
+
 else:
+
     @contextmanager  # noqa: T484
     def TemporaryFileName(*args, **kwargs):
         with tempfile.NamedTemporaryFile(*args, **kwargs) as f:
             yield f.name
 
+
 if IS_WINDOWS:
+
     @contextmanager
     def TemporaryDirectoryName(suffix=None):
         # On Windows the directory created by TemporaryDirectory is likely to be removed prematurely,
@@ -707,13 +863,18 @@ if IS_WINDOWS:
             yield dir_name
         finally:
             shutil.rmtree(dir_name)
+
+
 else:
+
     @contextmanager  # noqa: T484
     def TemporaryDirectoryName(suffix=None):
         with tempfile.TemporaryDirectory(suffix=suffix) as d:
             yield d
 
-IS_FILESYSTEM_UTF8_ENCODING = sys.getfilesystemencoding() == 'utf-8'
+
+IS_FILESYSTEM_UTF8_ENCODING = sys.getfilesystemencoding() == "utf-8"
+
 
 def _check_module_exists(name: str) -> bool:
     r"""Returns if a top-level module with :attr:`name` exists *without**
@@ -724,48 +885,50 @@ def _check_module_exists(name: str) -> b
     """
     try:
         import importlib.util
+
         spec = importlib.util.find_spec(name)
         return spec is not None
     except ImportError:
         return False
 
-TEST_NUMPY = _check_module_exists('numpy')
-TEST_SCIPY = _check_module_exists('scipy')
+
+TEST_NUMPY = _check_module_exists("numpy")
+TEST_SCIPY = _check_module_exists("scipy")
 TEST_MKL = torch.backends.mkl.is_available()
 TEST_CUDA = torch.cuda.is_available()
-TEST_NUMBA = _check_module_exists('numba')
+TEST_NUMBA = _check_module_exists("numba")
 
-TEST_DILL = _check_module_exists('dill')
+TEST_DILL = _check_module_exists("dill")
 
-TEST_LIBROSA = _check_module_exists('librosa')
+TEST_LIBROSA = _check_module_exists("librosa")
 
 BUILD_WITH_CAFFE2 = _check_module_exists("caffe2.python.caffe2_pybind11_state")
 
 # Python 2.7 doesn't have spawn
-NO_MULTIPROCESSING_SPAWN = os.environ.get('NO_MULTIPROCESSING_SPAWN', '0') == '1'
-TEST_WITH_ASAN = os.getenv('PYTORCH_TEST_WITH_ASAN', '0') == '1'
-TEST_WITH_DEV_DBG_ASAN = os.getenv('PYTORCH_TEST_WITH_DEV_DBG_ASAN', '0') == '1'
-TEST_WITH_TSAN = os.getenv('PYTORCH_TEST_WITH_TSAN', '0') == '1'
-TEST_WITH_UBSAN = os.getenv('PYTORCH_TEST_WITH_UBSAN', '0') == '1'
-TEST_WITH_ROCM = os.getenv('PYTORCH_TEST_WITH_ROCM', '0') == '1'
+NO_MULTIPROCESSING_SPAWN = os.environ.get("NO_MULTIPROCESSING_SPAWN", "0") == "1"
+TEST_WITH_ASAN = os.getenv("PYTORCH_TEST_WITH_ASAN", "0") == "1"
+TEST_WITH_DEV_DBG_ASAN = os.getenv("PYTORCH_TEST_WITH_DEV_DBG_ASAN", "0") == "1"
+TEST_WITH_TSAN = os.getenv("PYTORCH_TEST_WITH_TSAN", "0") == "1"
+TEST_WITH_UBSAN = os.getenv("PYTORCH_TEST_WITH_UBSAN", "0") == "1"
+TEST_WITH_ROCM = os.getenv("PYTORCH_TEST_WITH_ROCM", "0") == "1"
 
 # TODO: Remove PYTORCH_MIOPEN_SUGGEST_NHWC once ROCm officially supports NHWC in MIOpen
 # See #64427
-TEST_WITH_MIOPEN_SUGGEST_NHWC = os.getenv('PYTORCH_MIOPEN_SUGGEST_NHWC', '0') == '1'
+TEST_WITH_MIOPEN_SUGGEST_NHWC = os.getenv("PYTORCH_MIOPEN_SUGGEST_NHWC", "0") == "1"
 
 # Enables tests that are slow to run (disabled by default)
-TEST_WITH_SLOW = os.getenv('PYTORCH_TEST_WITH_SLOW', '0') == '1'
+TEST_WITH_SLOW = os.getenv("PYTORCH_TEST_WITH_SLOW", "0") == "1"
 
 # Disables non-slow tests (these tests enabled by default)
 # This is usually used in conjunction with TEST_WITH_SLOW to
 # run *only* slow tests.  (I could have done an enum, but
 # it felt a little awkward.
-TEST_SKIP_FAST = os.getenv('PYTORCH_TEST_SKIP_FAST', '0') == '1'
+TEST_SKIP_FAST = os.getenv("PYTORCH_TEST_SKIP_FAST", "0") == "1"
 
 # Disables noarch tests; all but one CI configuration disables these.  We don't
 # disable them for local runs because you still want to run them
 # (unlike slow tests!)
-TEST_SKIP_NOARCH = os.getenv('PYTORCH_TEST_SKIP_NOARCH', '0') == '1'
+TEST_SKIP_NOARCH = os.getenv("PYTORCH_TEST_SKIP_NOARCH", "0") == "1"
 
 # Determine whether to enable cuda memory leak check.
 # CUDA mem leak check is expensive and thus we don't want to execute it on every
@@ -773,27 +936,27 @@ TEST_SKIP_NOARCH = os.getenv('PYTORCH_TE
 # If this is True then CUDA memory leak checks are skipped. If this is false
 #   then CUDA memory leak checks are performed.
 # See: https://github.com/pytorch/pytorch/pull/59402#issuecomment-858811135
-TEST_SKIP_CUDA_MEM_LEAK_CHECK = os.getenv('PYTORCH_TEST_SKIP_CUDA_MEM_LEAK_CHECK', '0') == '1'
+TEST_SKIP_CUDA_MEM_LEAK_CHECK = os.getenv("PYTORCH_TEST_SKIP_CUDA_MEM_LEAK_CHECK", "0") == "1"
 
 # Disables tests for when on Github Actions
-ON_GHA = os.getenv('GITHUB_ACTIONS', '0') == '1'
+ON_GHA = os.getenv("GITHUB_ACTIONS", "0") == "1"
 
 # True if CI is running TBB-enabled Pytorch
 IS_TBB = "tbb" in os.getenv("BUILD_ENVIRONMENT", "")
 
 # Dict of NumPy dtype -> torch dtype (when the correspondence exists)
 numpy_to_torch_dtype_dict = {
-    np.bool_      : torch.bool,
-    np.uint8      : torch.uint8,
-    np.int8       : torch.int8,
-    np.int16      : torch.int16,
-    np.int32      : torch.int32,
-    np.int64      : torch.int64,
-    np.float16    : torch.float16,
-    np.float32    : torch.float32,
-    np.float64    : torch.float64,
-    np.complex64  : torch.complex64,
-    np.complex128 : torch.complex128
+    np.bool_: torch.bool,
+    np.uint8: torch.uint8,
+    np.int8: torch.int8,
+    np.int16: torch.int16,
+    np.int32: torch.int32,
+    np.int64: torch.int64,
+    np.float16: torch.float16,
+    np.float32: torch.float32,
+    np.float64: torch.float64,
+    np.complex64: torch.complex64,
+    np.complex128: torch.complex128,
 }
 
 
@@ -823,7 +986,8 @@ if IS_WINDOWS:
     numpy_to_torch_dtype_dict[np.intc] = torch.int
 
 # Dict of torch dtype -> NumPy dtype
-torch_to_numpy_dtype_dict = {value : key for (key, value) in numpy_to_torch_dtype_dict.items()}
+torch_to_numpy_dtype_dict = {value: key for (key, value) in numpy_to_torch_dtype_dict.items()}
+
 
 def skipIfRocm(fn):
     @wraps(fn)
@@ -832,9 +996,13 @@ def skipIfRocm(fn):
             raise unittest.SkipTest("test doesn't currently work on the ROCm stack")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
+
 # Skips a test on CUDA if ROCm is unavailable or its version is lower than requested.
+
+
 def skipIfRocmVersionLessThan(version=None):
     def dec_fn(fn):
         @wraps(fn)
@@ -843,15 +1011,18 @@ def skipIfRocmVersionLessThan(version=No
                 reason = "ROCm not available"
                 raise unittest.SkipTest(reason)
             rocm_version = str(torch.version.hip)
-            rocm_version = rocm_version.split("-")[0]    # ignore git sha
+            rocm_version = rocm_version.split("-")[0]  # ignore git sha
             rocm_version_tuple = tuple(int(x) for x in rocm_version.split("."))
             if rocm_version_tuple is None or version is None or rocm_version_tuple < tuple(version):
                 reason = "ROCm {0} is available but {1} required".format(rocm_version_tuple, version)
                 raise unittest.SkipTest(reason)
             return fn(self, *args, **kwargs)
+
         return wrap_fn
+
     return dec_fn
 
+
 def skipIfNotMiopenSuggestNHWC(fn):
     @wraps(fn)
     def wrapper(*args, **kwargs):
@@ -859,10 +1030,14 @@ def skipIfNotMiopenSuggestNHWC(fn):
             raise unittest.SkipTest("test doesn't currently work without MIOpen NHWC activation")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
+
 # Context manager for setting deterministic flag and automatically
 # resetting it to its original value
+
+
 class DeterministicGuard:
     def __init__(self, deterministic, *, warn_only=False):
         self.deterministic = deterministic
@@ -871,19 +1046,18 @@ class DeterministicGuard:
     def __enter__(self):
         self.deterministic_restore = torch.are_deterministic_algorithms_enabled()
         self.warn_only_restore = torch.is_deterministic_algorithms_warn_only_enabled()
-        torch.use_deterministic_algorithms(
-            self.deterministic,
-            warn_only=self.warn_only)
+        torch.use_deterministic_algorithms(self.deterministic, warn_only=self.warn_only)
 
     def __exit__(self, exception_type, exception_value, traceback):
-        torch.use_deterministic_algorithms(
-            self.deterministic_restore,
-            warn_only=self.warn_only_restore)
+        torch.use_deterministic_algorithms(self.deterministic_restore, warn_only=self.warn_only_restore)
+
 
 # Context manager for setting cuda sync debug mode and reset it
 # to original value
 # we are not exposing it to the core because sync debug mode is
 # global and thus not thread safe
+
+
 class CudaSyncGuard:
     def __init__(self, sync_debug_mode):
         self.mode = sync_debug_mode
@@ -895,6 +1069,7 @@ class CudaSyncGuard:
     def __exit__(self, exception_type, exception_value, traceback):
         torch.cuda.set_sync_debug_mode(self.debug_mode_restore)
 
+
 # This decorator can be used for API tests that call
 # torch.use_deterministic_algorithms().  When the test is finished, it will
 # restore the previous deterministic flag setting.
@@ -924,22 +1099,26 @@ class CudaSyncGuard:
 #       error_message = e.output.decode('utf-8')
 #       # Handle exceptions raised by the subprocess here
 #
+
+
 def wrapDeterministicFlagAPITest(fn):
     @wraps(fn)
     def wrapper(*args, **kwargs):
         with DeterministicGuard(
-                torch.are_deterministic_algorithms_enabled(),
-                warn_only=torch.is_deterministic_algorithms_warn_only_enabled()):
+            torch.are_deterministic_algorithms_enabled(),
+            warn_only=torch.is_deterministic_algorithms_warn_only_enabled(),
+        ):
+
             class CuBLASConfigGuard:
-                cublas_var_name = 'CUBLAS_WORKSPACE_CONFIG'
+                cublas_var_name = "CUBLAS_WORKSPACE_CONFIG"
 
                 def __enter__(self):
-                    self.is_cuda10_2_or_higher = (
-                        (torch.version.cuda is not None)
-                        and ([int(x) for x in torch.version.cuda.split(".")] >= [10, 2]))
+                    self.is_cuda10_2_or_higher = (torch.version.cuda is not None) and (
+                        [int(x) for x in torch.version.cuda.split(".")] >= [10, 2]
+                    )
                     if self.is_cuda10_2_or_higher:
                         self.cublas_config_restore = os.environ.get(self.cublas_var_name)
-                        os.environ[self.cublas_var_name] = ':4096:8'
+                        os.environ[self.cublas_var_name] = ":4096:8"
 
                 def __exit__(self, exception_type, exception_value, traceback):
                     if self.is_cuda10_2_or_higher:
@@ -949,10 +1128,13 @@ def wrapDeterministicFlagAPITest(fn):
                                 del os.environ[self.cublas_var_name]
                         else:
                             os.environ[self.cublas_var_name] = self.cublas_config_restore
+
             with CuBLASConfigGuard():
                 fn(*args, **kwargs)
+
     return wrapper
 
+
 def skipIfCompiledWithoutNumpy(fn):
     # Even if the numpy module is present, if `USE_NUMPY=0` is used during the
     # build, numpy tests will fail
@@ -971,11 +1153,14 @@ def skipIfCompiledWithoutNumpy(fn):
             raise unittest.SkipTest("PyTorch was compiled without numpy support")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
+
 def _test_function(fn, device):
     def run_test_function(self):
         return fn(self, device)
+
     return run_test_function
 
 
@@ -983,9 +1168,10 @@ def skipIfNoLapack(fn):
     @wraps(fn)
     def wrapper(*args, **kwargs):
         if not torch._C.has_lapack:
-            raise unittest.SkipTest('PyTorch compiled without Lapack')
+            raise unittest.SkipTest("PyTorch compiled without Lapack")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
 
@@ -1004,8 +1190,8 @@ def skipIfNotRegistered(op_name, message
         return unittest.skip("Pytorch is compiled without Caffe2")
     try:
         from caffe2.python import core
-        skipper = unittest.skipIf(op_name not in core._REGISTERED_OPERATORS,
-                                  message)
+
+        skipper = unittest.skipIf(op_name not in core._REGISTERED_OPERATORS, message)
     except ImportError:
         skipper = unittest.skip("Cannot import `caffe2.python.core`")
     return skipper
@@ -1018,6 +1204,7 @@ def skipIfNoSciPy(fn):
             raise unittest.SkipTest("test require SciPy, but SciPy not found")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
 
@@ -1028,6 +1215,7 @@ def skipIfOnGHA(fn):
             raise unittest.SkipTest("Test disabled for GHA")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
 
@@ -1039,7 +1227,9 @@ def skipIfTBB(message="This test makes T
                 raise unittest.SkipTest(message)
             else:
                 fn(*args, **kwargs)
+
         return wrapper
+
     return dec_fn
 
 
@@ -1050,7 +1240,8 @@ def slowTest(fn):
             raise unittest.SkipTest("test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test")
         else:
             fn(*args, **kwargs)
-    wrapper.__dict__['slow_test'] = True
+
+    wrapper.__dict__["slow_test"] = True
     return wrapper
 
 
@@ -1065,34 +1256,40 @@ def noarchTest(fn):
             raise unittest.SkipTest("test is noarch: we are skipping noarch tests due to TEST_SKIP_NOARCH")
         else:
             fn(*args, **kwargs)
+
     return wrapper
 
 
 def slowAwareTest(fn):
-    fn.__dict__['slow_test'] = True
+    fn.__dict__["slow_test"] = True
     return fn
 
 
 def skipCUDAMemoryLeakCheckIf(condition):
     def dec(fn):
-        if getattr(fn, '_do_cuda_memory_leak_check', True):  # if current True
+        if getattr(fn, "_do_cuda_memory_leak_check", True):  # if current True
             fn._do_cuda_memory_leak_check = not condition
         return fn
+
     return dec
 
+
 def skipCUDANonDefaultStreamIf(condition):
     def dec(fn):
-        if getattr(fn, '_do_cuda_non_default_stream', True):  # if current True
+        if getattr(fn, "_do_cuda_non_default_stream", True):  # if current True
             fn._do_cuda_non_default_stream = not condition
         return fn
+
     return dec
 
+
 def suppress_warnings(fn):
     @wraps(fn)
     def wrapper(*args, **kwargs):
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
             fn(*args, **kwargs)
+
     return wrapper
 
 
@@ -1145,6 +1342,7 @@ def freeze_rng_state():
                 torch.cuda.set_rng_state(cuda_rng_state)
             torch.set_rng_state(rng_state)
 
+
 @contextlib.contextmanager
 def set_default_dtype(dtype):
     saved_dtype = torch.get_default_dtype()
@@ -1154,6 +1352,7 @@ def set_default_dtype(dtype):
     finally:
         torch.set_default_dtype(saved_dtype)
 
+
 def iter_indices(tensor):
     if tensor.dim() == 0:
         return range(0)
@@ -1193,7 +1392,7 @@ def is_iterable_of_tensors(iterable, inc
     return True
 
 
-class CudaNonDefaultStream():
+class CudaNonDefaultStream:
     def __enter__(self):
         # Before starting CUDA test save currently active streams on all
         # CUDA devices and set new non default streams to all CUDA devices
@@ -1214,7 +1413,8 @@ class CudaNonDefaultStream():
             torch._C._cuda_setStream(self.beforeStreams[d]._cdata)
         torch._C._cuda_setDevice(beforeDevice)
 
-class CudaMemoryLeakCheck():
+
+class CudaMemoryLeakCheck:
     def __init__(self, testcase, name=None):
         self.name = testcase.id() if name is None else name
         self.testcase = testcase
@@ -1222,6 +1422,7 @@ class CudaMemoryLeakCheck():
         # initialize context & RNG to prevent false positive detections
         # when the test is the first to initialize those
         from torch.testing._internal.common_cuda import initialize_cuda_context_rng
+
         initialize_cuda_context_rng()
 
     # Stores CUDA memory data provided by PyTorch's caching allocator and
@@ -1303,31 +1504,37 @@ class CudaMemoryLeakCheck():
                 # NOTE: this may be a problem with how the caching allocator collects its
                 #   statistics or a leak too small to trigger the allocation of an
                 #   additional block of memory by the CUDA driver
-                msg = ("CUDA caching allocator reports a memory leak not "
-                       "verified by the driver API in {}! "
-                       "Caching allocator allocated memory was {} and is now reported as {} "
-                       "on device {}. "
-                       "CUDA driver allocated memory was {} and is now {}.").format(
+                msg = (
+                    "CUDA caching allocator reports a memory leak not "
+                    "verified by the driver API in {}! "
+                    "Caching allocator allocated memory was {} and is now reported as {} "
+                    "on device {}. "
+                    "CUDA driver allocated memory was {} and is now {}."
+                ).format(
                     self.name,
                     self.caching_allocator_befores[i],
                     caching_allocator_mem_allocated,
                     i,
                     self.driver_befores[i],
-                    driver_mem_allocated)
+                    driver_mem_allocated,
+                )
                 warnings.warn(msg)
             elif caching_allocator_discrepancy and driver_discrepancy:
                 # A caching allocator discrepancy validated by the driver API is a
                 #   failure (except on ROCm, see below)
-                msg = ("CUDA driver API confirmed a leak in {}! "
-                       "Caching allocator allocated memory was {} and is now reported as {} "
-                       "on device {}. "
-                       "CUDA driver allocated memory was {} and is now {}.").format(
+                msg = (
+                    "CUDA driver API confirmed a leak in {}! "
+                    "Caching allocator allocated memory was {} and is now reported as {} "
+                    "on device {}. "
+                    "CUDA driver allocated memory was {} and is now {}."
+                ).format(
                     self.name,
                     self.caching_allocator_befores[i],
                     caching_allocator_mem_allocated,
                     i,
                     self.driver_befores[i],
-                    driver_mem_allocated)
+                    driver_mem_allocated,
+                )
 
                 # See #62533
                 # ROCM: Sometimes the transient memory is reported as leaked memory
@@ -1336,6 +1543,7 @@ class CudaMemoryLeakCheck():
                 else:
                     raise RuntimeError(msg)
 
+
 @contextmanager
 def skip_exception_type(exc_type):
     try:
@@ -1343,17 +1551,17 @@ def skip_exception_type(exc_type):
     except exc_type as e:
         raise unittest.SkipTest(f"not implemented: {e}") from e
 
+
 #  "min_satisfying_examples" setting has been deprecated in hypythesis
 #  3.56.0 and removed in hypothesis 4.x
 try:
     import hypothesis
 
     def settings(*args, **kwargs):
-        if 'min_satisfying_examples' in kwargs and hypothesis.version.__version_info__ >= (3, 56, 0):
-            kwargs.pop('min_satisfying_examples')
+        if "min_satisfying_examples" in kwargs and hypothesis.version.__version_info__ >= (3, 56, 0):
+            kwargs.pop("min_satisfying_examples")
         return hypothesis.settings(*args, **kwargs)
 
-
     hypothesis.settings.register_profile(
         "pytorch_ci",
         settings(
@@ -1361,35 +1569,42 @@ try:
             suppress_health_check=[hypothesis.HealthCheck.too_slow],
             database=None,
             max_examples=50,
-            verbosity=hypothesis.Verbosity.normal))
+            verbosity=hypothesis.Verbosity.normal,
+        ),
+    )
     hypothesis.settings.register_profile(
         "dev",
         settings(
             suppress_health_check=[hypothesis.HealthCheck.too_slow],
             database=None,
             max_examples=10,
-            verbosity=hypothesis.Verbosity.normal))
+            verbosity=hypothesis.Verbosity.normal,
+        ),
+    )
     hypothesis.settings.register_profile(
         "debug",
         settings(
             suppress_health_check=[hypothesis.HealthCheck.too_slow],
             database=None,
             max_examples=1000,
-            verbosity=hypothesis.Verbosity.verbose))
-
-    hypothesis.settings.load_profile(
-        "pytorch_ci" if IS_IN_CI else os.getenv('PYTORCH_HYPOTHESIS_PROFILE', 'dev')
+            verbosity=hypothesis.Verbosity.verbose,
+        ),
     )
+
+    hypothesis.settings.load_profile("pytorch_ci" if IS_IN_CI else os.getenv("PYTORCH_HYPOTHESIS_PROFILE", "dev"))
 except ImportError:
-    print('Fail to import hypothesis in common_utils, tests are not derandomized')
+    print("Fail to import hypothesis in common_utils, tests are not derandomized")
 
 # Used in check_if_enable to see if a test method should be disabled by an issue,
 # sanitizes a test method name from appended suffixes by @dtypes parametrization.
 # e.g., an issue with title "DISABLED test_bitwise_ops (__main__.TestBinaryUfuncs)" should
 # disabled ALL parametrized test_bitwise_ops tests, such test_bitwise_ops_cuda_int32
+
+
 def remove_device_and_dtype_suffixes(test_name: str) -> str:
     # import statement is localized to avoid circular dependency issues with common_device_type.py
     from torch.testing._internal.common_device_type import get_device_type_test_bases
+
     device_suffixes = [x.device_type for x in get_device_type_test_bases()]
     dtype_suffixes = [str(dt)[len("torch."):] for dt in get_all_dtypes()]
 
@@ -1402,10 +1617,10 @@ def remove_device_and_dtype_suffixes(tes
 
 
 def check_if_enable(test: unittest.TestCase):
-    test_suite = str(test.__class__).split('\'')[1]
-    raw_test_name = f'{test._testMethodName} ({test_suite})'
+    test_suite = str(test.__class__).split("'")[1]
+    raw_test_name = f"{test._testMethodName} ({test_suite})"
     if slow_tests_dict is not None and raw_test_name in slow_tests_dict:
-        getattr(test, test._testMethodName).__dict__['slow_test'] = True
+        getattr(test, test._testMethodName).__dict__["slow_test"] = True
         if not TEST_WITH_SLOW:
             raise unittest.SkipTest("test is slow; run with PYTORCH_TEST_WITH_SLOW to enable test")
     sanitized_test_method_name = remove_device_and_dtype_suffixes(test._testMethodName)
@@ -1417,8 +1632,9 @@ def check_if_enable(test: unittest.TestC
                 disabled_test_suite = disable_test_parts[1][1:-1]
                 # if test method name or its sanitized version exactly matches the disabled test method name
                 # AND allow non-parametrized suite names to disable parametrized ones (TestSuite disables TestSuiteCPU)
-                if (test._testMethodName == disabled_test_name or sanitized_test_method_name == disabled_test_name) \
-                   and disabled_test_suite in test_suite:
+                if (
+                    test._testMethodName == disabled_test_name or sanitized_test_method_name == disabled_test_name
+                ) and disabled_test_suite in test_suite:
                     platform_to_conditional: Dict = {
                         "mac": IS_MACOS,
                         "macos": IS_MACOS,
@@ -1426,16 +1642,18 @@ def check_if_enable(test: unittest.TestC
                         "windows": IS_WINDOWS,
                         "linux": IS_LINUX,
                         "rocm": TEST_WITH_ROCM,
-                        "asan": TEST_WITH_ASAN
+                        "asan": TEST_WITH_ASAN,
                     }
                     if platforms == [] or any([platform_to_conditional[platform] for platform in platforms]):
-                        skip_msg = f"Test is disabled because an issue exists disabling it: {issue_url}" \
-                            f" for {'all' if platforms == [] else ''}platform(s) {', '.join(platforms)}. " \
-                            "If you're seeing this on your local machine and would like to enable this test, " \
+                        skip_msg = (
+                            f"Test is disabled because an issue exists disabling it: {issue_url}"
+                            f" for {'all' if platforms == [] else ''}platform(s) {', '.join(platforms)}. "
+                            "If you're seeing this on your local machine and would like to enable this test, "
                             "please make sure IN_CI is not set and you are not using the flag --import-disabled-tests."
+                        )
                         raise unittest.SkipTest(skip_msg)
     if TEST_SKIP_FAST:
-        if not getattr(test, test._testMethodName).__dict__.get('slow_test', False):
+        if not getattr(test, test._testMethodName).__dict__.get("slow_test", False):
             raise unittest.SkipTest("test is fast; we disabled it with PYTORCH_TEST_SKIP_FAST")
 
 
@@ -1447,12 +1665,14 @@ def check_if_enable(test: unittest.TestC
 # change the supported inputs, but the comparison logic is the same.
 # TODO: Revisit the relaxed pairs and check how much work it is to fix the tests that would fail without the relaxation.
 
+
 class RelaxedBooleanPair(BooleanPair):
     """Pair for boolean-like inputs.
 
     In contrast to the builtin :class:`BooleanPair`, this class also supports one input being a number or a single
     element tensor-like.
     """
+
     _supported_number_types = NumberPair(0, 0)._supported_types
 
     def _process_inputs(self, actual, expected, *, id):
@@ -1499,6 +1719,7 @@ class RelaxedNumberPair(NumberPair):
     supports overriding the absolute and relative tolerance through the ``@precisionOverride`` and
     ``@toleranceOverride`` decorators.
     """
+
     _TYPE_TO_DTYPE = {
         int: torch.int64,
         float: torch.float32,
@@ -1506,7 +1727,7 @@ class RelaxedNumberPair(NumberPair):
     }
 
     def __init__(
-            self, actual, expected, *, rtol_override=0.0, atol_override=0.0, check_dtype=None, **other_parameters
+        self, actual, expected, *, rtol_override=0.0, atol_override=0.0, check_dtype=None, **other_parameters
     ) -> None:
         super().__init__(actual, expected, check_dtype=False, **other_parameters)
         self.rtol = max(self.rtol, rtol_override)
@@ -1518,8 +1739,8 @@ class RelaxedNumberPair(NumberPair):
         tensor_or_array_types: Tuple[Type, ...] = (torch.Tensor, np.ndarray)
         other_supported_types = (*self._supported_types, *tensor_or_array_types)
         if not (
-                (isinstance(actual, self._supported_types) and isinstance(expected, other_supported_types))
-                or (isinstance(expected, self._supported_types) and isinstance(actual, other_supported_types))
+            (isinstance(actual, self._supported_types) and isinstance(expected, other_supported_types))
+            or (isinstance(expected, self._supported_types) and isinstance(actual, other_supported_types))
         ):
             raise UnsupportedInputs()
 
@@ -1557,6 +1778,7 @@ class TensorOrArrayPair(TensorLikePair):
     In addition, this class supports overriding the absolute and relative tolerance through the ``@precisionOverride``
     and ``@toleranceOverride`` decorators.
     """
+
     def __init__(self, actual, expected, *, rtol_override=0.0, atol_override=0.0, **other_parameters):
         super().__init__(actual, expected, **other_parameters)
         self.rtol = max(self.rtol, rtol_override)
@@ -1588,6 +1810,7 @@ class UnittestPair(Pair):
 
     Define the :attr:`UnittestPair.CLS` in a subclass to indicate which class(es) of the inputs the pair should support.
     """
+
     CLS: Union[Type, Tuple[Type, ...]]
     TYPE_NAME: Optional[str] = None
 
@@ -1695,20 +1918,20 @@ class TestCase(expecttest.TestCase):
     # the test, skip it instead.
     _ignore_not_implemented_error = False
 
-    def __init__(self, method_name='runTest'):
+    def __init__(self, method_name="runTest"):
         super().__init__(method_name)
 
         test_method = getattr(self, method_name, None)
         if test_method is not None:
             # Wraps the tested method if we should do CUDA memory check.
             if not TEST_SKIP_CUDA_MEM_LEAK_CHECK:
-                self._do_cuda_memory_leak_check &= getattr(test_method, '_do_cuda_memory_leak_check', True)
+                self._do_cuda_memory_leak_check &= getattr(test_method, "_do_cuda_memory_leak_check", True)
                 # FIXME: figure out the flaky -1024 anti-leaks on windows. See #8044
                 if self._do_cuda_memory_leak_check and not IS_WINDOWS:
                     self.wrap_with_cuda_policy(method_name, self.assertLeaksNoCudaTensors)
 
             # Wraps the tested method if we should enforce non default CUDA stream.
-            self._do_cuda_non_default_stream &= getattr(test_method, '_do_cuda_non_default_stream', True)
+            self._do_cuda_non_default_stream &= getattr(test_method, "_do_cuda_non_default_stream", True)
             if self._do_cuda_non_default_stream and not IS_WINDOWS:
                 self.wrap_with_cuda_policy(method_name, self.enforceNonDefaultStream)
 
@@ -1730,8 +1953,9 @@ class TestCase(expecttest.TestCase):
         # TODO: sure looks like we unconditionally initialize the context here
         # -- ezyang
         from torch.testing._internal.common_cuda import TEST_CUDA
+
         fullname = self.id().lower()  # class_name.method_name
-        if TEST_CUDA and ('gpu' in fullname or 'cuda' in fullname):
+        if TEST_CUDA and ("gpu" in fullname or "cuda" in fullname):
             setattr(self, method_name, self.wrap_method_with_policy(test_method, policy))
 
     def wrap_with_policy(self, method_name, policy):
@@ -1752,6 +1976,7 @@ class TestCase(expecttest.TestCase):
         def wrapper(self, *args, **kwargs):
             with policy():
                 method(*args, **kwargs)
+
         return types.MethodType(wrapper, self)
 
     def wrap_with_cuda_memory_check(self, method):
@@ -1769,7 +1994,8 @@ class TestCase(expecttest.TestCase):
         using_unittest = isinstance(result, unittest.TestResult)
 
         if using_unittest:
-            failures_before = 0 if result is None else len(result.failures)  # num tests marked as failed before starting
+            # num tests marked as failed before starting
+            failures_before = 0 if result is None else len(result.failures)
             errors_before = 0 if result is None else len(result.errors)  # num tests marked as errored before starting
 
         super().run(result=result)
@@ -1781,6 +2007,7 @@ class TestCase(expecttest.TestCase):
                     # This is a big hacky, XMLRunner modifies expected type from TestCase to TestInfo
                     # Create dummy TestInfo to record results correctly
                     from xmlrunner.result import _TestInfo  # type: ignore[import]
+
                     case = _TestInfo(result, case)
                     case.output = _TestInfo.ERROR
                     case.elapsed_time = 0.0
@@ -1813,7 +2040,6 @@ class TestCase(expecttest.TestCase):
             result.addUnexpectedSuccess(self)
             self._run_with_retry(result=result, num_runs_left=num_retries_left, report_only=report_only)
 
-
     def run(self, result=None):
         num_runs = MAX_NUM_RETRIES + 1 if RETRY_TEST_CASES else 1
         self._run_with_retry(result=result, num_runs_left=num_runs, report_only=not OVERRIDE_FLAKY_SIGNAL)
@@ -1823,8 +2049,7 @@ class TestCase(expecttest.TestCase):
         set_rng_seed(SEED)
 
     @staticmethod
-    def _make_crow_indices(n_rows, n_cols, nnz,
-                           *, device, dtype, random=True):
+    def _make_crow_indices(n_rows, n_cols, nnz, *, device, dtype, random=True):
         """Return crow_indices of a CSR tensor with size (n_rows, n_cols) and
         the number of specified elements nnz.
 
@@ -1889,7 +2114,7 @@ class TestCase(expecttest.TestCase):
 
         # Different from the original method description, here counts
         # has leading 0 required by crow_indices:
-        counts = torch.zeros(n_rows + 1, dtype=dtype, device=torch.device('cpu'))
+        counts = torch.zeros(n_rows + 1, dtype=dtype, device=torch.device("cpu"))
 
         n = m = 0
         N = sawteeth(n, m)
@@ -1929,12 +2154,11 @@ class TestCase(expecttest.TestCase):
             m, N = m_right, N_right
             # fill the bottom rectangle with counts:
             assert m
-            counts[1:n_rows - n + 1].fill_(m)
+            counts[1: n_rows - n + 1].fill_(m)
 
         if N:
             # fill the sawteeth window with counts
-            q, r = divmod(nnz - n * n_cols - m * (n_rows - n),
-                          (n_cols - m) * (n_cols - m + 1) // 2)
+            q, r = divmod(nnz - n * n_cols - m * (n_rows - n), (n_cols - m) * (n_cols - m + 1) // 2)
             p = 1 + q * (n_cols - m + 1)
             if sys.version_info >= (3, 8):
                 k = math.isqrt(2 * r)
@@ -1952,7 +2176,7 @@ class TestCase(expecttest.TestCase):
             # sequence of full sawteeth:
             counts[1:p] = torch.arange(p - 1, dtype=dtype, device=counts.device) % (n_cols - m + 1)
             # incomplete sawtooth:
-            counts[p:p + k + 1] += torch.arange(k + 1, dtype=dtype, device=counts.device)
+            counts[p: p + k + 1] += torch.arange(k + 1, dtype=dtype, device=counts.device)
         else:
             # given input does not support sawteeth
             p = 1
@@ -1974,7 +2198,7 @@ class TestCase(expecttest.TestCase):
 
     def genSparseCSRTensor(self, size, nnz, *, device, dtype, index_dtype):
         sparse_dim = 2
-        assert all(size[d] > 0 for d in range(sparse_dim)) or nnz == 0, 'invalid arguments'
+        assert all(size[d] > 0 for d in range(sparse_dim)) or nnz == 0, "invalid arguments"
         assert len(size) == sparse_dim
 
         def random_sparse_csr(n_rows, n_cols, nnz):
@@ -1982,22 +2206,21 @@ class TestCase(expecttest.TestCase):
             col_indices = torch.zeros(nnz, dtype=index_dtype, device=device)
             for i in range(n_rows):
                 count = crow_indices[i + 1] - crow_indices[i]
-                col_indices[crow_indices[i]:crow_indices[i + 1]], _ = torch.sort(
-                    torch.randperm(n_cols, dtype=index_dtype, device=device)[:count])
+                col_indices[crow_indices[i]: crow_indices[i + 1]], _ = torch.sort(
+                    torch.randperm(n_cols, dtype=index_dtype, device=device)[:count]
+                )
             low = -1 if dtype != torch.uint8 else 0
             high = 1 if dtype != torch.uint8 else 2
             values = make_tensor([nnz], device=device, dtype=dtype, low=low, high=high)
             return values, crow_indices, col_indices
 
         values, crow_indices, col_indices = random_sparse_csr(size[0], size[1], nnz)
-        return torch.sparse_csr_tensor(crow_indices,
-                                       col_indices,
-                                       values, size=size, dtype=dtype, device=device)
+        return torch.sparse_csr_tensor(crow_indices, col_indices, values, size=size, dtype=dtype, device=device)
 
     def genSparseTensor(self, size, sparse_dim, nnz, is_uncoalesced, device, dtype):
         # Assert not given impossible combination, where the sparse dims have
         # empty numel, but nnz > 0 makes the indices containing values.
-        assert all(size[d] > 0 for d in range(sparse_dim)) or nnz == 0, 'invalid arguments'
+        assert all(size[d] > 0 for d in range(sparse_dim)) or nnz == 0, "invalid arguments"
 
         v_size = [nnz] + list(size[sparse_dim:])
         v = make_tensor(v_size, device=device, dtype=dtype, low=-1, high=1)
@@ -2040,8 +2263,7 @@ class TestCase(expecttest.TestCase):
     #   tensor (array). If the torch and/or NumPy function require additional
     #   arguments then wrap the function in a lambda or pass a partial function.
     # TODO: add args/kwargs for passing to assertEqual (e.g. rtol, atol)
-    def compare_with_numpy(self, torch_fn, np_fn, tensor_like,
-                           device=None, dtype=None, **kwargs):
+    def compare_with_numpy(self, torch_fn, np_fn, tensor_like, device=None, dtype=None, **kwargs):
         assert TEST_NUMPY
 
         if isinstance(tensor_like, torch.Tensor):
@@ -2080,20 +2302,20 @@ class TestCase(expecttest.TestCase):
         return self.assertEqual(*args, exact_dtype=False, **kwargs)
 
     def assertEqual(
-            self,
-            x,
-            y,
-            msg: Optional[str] = None,
-            *,
-            atol: Optional[float] = None,
-            rtol: Optional[float] = None,
-            equal_nan=True,
-            exact_dtype=True,
-            # TODO: default this to True
-            exact_device=False,
-            exact_layout=False,
-            exact_stride=False,
-            exact_is_coalesced=False
+        self,
+        x,
+        y,
+        msg: Optional[str] = None,
+        *,
+        atol: Optional[float] = None,
+        rtol: Optional[float] = None,
+        equal_nan=True,
+        exact_dtype=True,
+        # TODO: default this to True
+        exact_device=False,
+        exact_layout=False,
+        exact_stride=False,
+        exact_is_coalesced=False,
     ):
         # Hide this function from `pytest`'s traceback
         __tracebackhide__ = True
@@ -2102,9 +2324,8 @@ class TestCase(expecttest.TestCase):
         # back to an elementwise comparison. Note that this has to happen here and not for example in
         # `TensorOrArrayPair`, since at that stage we can no longer split the array into its elements and perform
         # multiple comparisons.
-        if any(
-            isinstance(input, np.ndarray) and not has_corresponding_torch_dtype(input.dtype) for input in (x, y)
-        ):
+        if any(isinstance(input, np.ndarray) and not has_corresponding_torch_dtype(input.dtype) for input in (x, y)):
+
             def to_list(input):
                 return input.tolist() if isinstance(input, (torch.Tensor, np.ndarray)) else list(input)
 
@@ -2154,8 +2375,16 @@ class TestCase(expecttest.TestCase):
             msg=msg,
         )
 
-    def assertNotEqual(self, x, y, msg: Optional[str] = None, *,                                       # type: ignore[override]
-                       atol: Optional[float] = None, rtol: Optional[float] = None, **kwargs) -> None:
+    def assertNotEqual(
+        self,
+        x,
+        y,
+        msg: Optional[str] = None,
+        *,  # type: ignore[override]
+        atol: Optional[float] = None,
+        rtol: Optional[float] = None,
+        **kwargs,
+    ) -> None:
         with self.assertRaises(AssertionError, msg=msg):
             self.assertEqual(x, y, msg, atol=atol, rtol=rtol, **kwargs)
 
@@ -2175,10 +2404,13 @@ class TestCase(expecttest.TestCase):
     # _ignore_not_implemented_error is True
     def assertRaises(self, expected_exception, *args, **kwargs):
         if self._ignore_not_implemented_error:
-            context: Optional[AssertRaisesContextIgnoreNotImplementedError] = \
-                AssertRaisesContextIgnoreNotImplementedError(expected_exception, self)  # type: ignore[call-arg]
+            context: Optional[
+                AssertRaisesContextIgnoreNotImplementedError
+            ] = AssertRaisesContextIgnoreNotImplementedError(
+                expected_exception, self
+            )  # type: ignore[call-arg]
             try:
-                return context.handle('assertRaises', args, kwargs)  # type: ignore[union-attr]
+                return context.handle("assertRaises", args, kwargs)  # type: ignore[union-attr]
             finally:
                 # see https://bugs.python.org/issue23890
                 context = None
@@ -2196,14 +2428,15 @@ class TestCase(expecttest.TestCase):
         # Checks whether the test is instantiated for a device type by testing
         # if the test class has defined the device_type attribute and,
         # if so, tests whether the instantiated device type is native or not
-        if hasattr(self, 'device_type') and self.device_type not in NATIVE_DEVICES:  # type: ignore[attr-defined]
+        if hasattr(self, "device_type") and self.device_type not in NATIVE_DEVICES:  # type: ignore[attr-defined]
             # empty string matches any string
-            expected_regex = ''
+            expected_regex = ""
 
         if self._ignore_not_implemented_error:
             context = AssertRaisesContextIgnoreNotImplementedError(  # type: ignore[call-arg]
-                expected_exception, self, expected_regex)
-            return context.handle('assertRaisesRegex', args, kwargs)  # type: ignore[attr-defined]
+                expected_exception, self, expected_regex
+            )
+            return context.handle("assertRaisesRegex", args, kwargs)  # type: ignore[attr-defined]
         else:
             return super().assertRaisesRegex(expected_exception, expected_regex, *args, **kwargs)
 
@@ -2212,9 +2445,9 @@ class TestCase(expecttest.TestCase):
     # If you need it, manually apply your callable in a lambda instead.
     def assertExpectedRaises(self, exc_type, callable, *args, **kwargs):
         subname = None
-        if 'subname' in kwargs:
-            subname = kwargs['subname']
-            del kwargs['subname']
+        if "subname" in kwargs:
+            subname = kwargs["subname"]
+            del kwargs["subname"]
         try:
             callable(*args, **kwargs)
         except exc_type as e:
@@ -2223,7 +2456,7 @@ class TestCase(expecttest.TestCase):
         # Don't put this in the try block; the AssertionError will catch it
         self.fail(msg="Did not raise when expected to")
 
-    def assertNotWarn(self, callable, msg=''):
+    def assertNotWarn(self, callable, msg=""):
         r"""
         Test if :attr:`callable` does not raise a warning.
         """
@@ -2234,7 +2467,7 @@ class TestCase(expecttest.TestCase):
             self.assertTrue(len(ws) == 0, msg)
 
     @contextmanager
-    def assertWarnsOnceRegex(self, category, regex=''):
+    def assertWarnsOnceRegex(self, category, regex=""):
         """Context manager for code that *must always* warn
 
         This filters expected warnings from the test and fails if
@@ -2247,11 +2480,12 @@ class TestCase(expecttest.TestCase):
             with set_warn_always_context(True):
                 yield
             if len(ws) == 0:
-                self.fail('no warning caught')
+                self.fail("no warning caught")
             self.assertTrue(any([type(w.message) is category for w in ws]))
             self.assertTrue(
                 any([re.match(pattern, str(w.message)) for w in ws]),
-                f'{pattern}, {[w.message for w in ws if type(w.message) is category]}')
+                f"{pattern}, {[w.message for w in ws if type(w.message) is category]}",
+            )
 
     def assertExpected(self, s, subname=None):
         r"""
@@ -2271,6 +2505,7 @@ class TestCase(expecttest.TestCase):
             if text.startswith(prefix):
                 return text[len(prefix):]
             return text
+
         # NB: we take __file__ from the module that defined the test
         # class, so we place the expect directory where the test script
         # lives, NOT where test/common_utils.py lives.  This doesn't matter in
@@ -2279,9 +2514,7 @@ class TestCase(expecttest.TestCase):
         module_id = self.__class__.__module__
         munged_id = remove_prefix(self.id(), module_id + ".")
         test_file = os.path.realpath(sys.modules[module_id].__file__)
-        expected_file = os.path.join(os.path.dirname(test_file),
-                                     "expect",
-                                     munged_id)
+        expected_file = os.path.join(os.path.dirname(test_file), "expect", munged_id)
 
         subname_output = ""
         if subname:
@@ -2292,10 +2525,9 @@ class TestCase(expecttest.TestCase):
 
         def accept_output(update_type):
             print("Accepting {} for {}{}:\n\n{}".format(update_type, munged_id, subname_output, s))
-            with open(expected_file, 'w') as f:
+            with open(expected_file, "w") as f:
                 # Adjust for producer_version, leave s unmodified
-                s_tag = re.sub(r'(producer_version): "[0-9.]*"',
-                               r'\1: "CURRENT_VERSION"', s)
+                s_tag = re.sub(r'(producer_version): "[0-9.]*"', r'\1: "CURRENT_VERSION"', s)
                 f.write(s_tag)
 
         try:
@@ -2308,19 +2540,21 @@ class TestCase(expecttest.TestCase):
                 return accept_output("output")
             else:
                 raise RuntimeError(
-                    ("I got this output for {}{}:\n\n{}\n\n"
-                     "No expect file exists; to accept the current output, run:\n"
-                     "python {} {} --accept").format(munged_id, subname_output, s, __main__.__file__, munged_id)) from None
+                    (
+                        "I got this output for {}{}:\n\n{}\n\n"
+                        "No expect file exists; to accept the current output, run:\n"
+                        "python {} {} --accept"
+                    ).format(munged_id, subname_output, s, __main__.__file__, munged_id)
+                ) from None
 
         # a hack for JIT tests
         if IS_WINDOWS:
-            expected = re.sub(r'CppOp\[(.+?)\]', 'CppOp[]', expected)
-            s = re.sub(r'CppOp\[(.+?)\]', 'CppOp[]', s)
+            expected = re.sub(r"CppOp\[(.+?)\]", "CppOp[]", expected)
+            s = re.sub(r"CppOp\[(.+?)\]", "CppOp[]", s)
 
         # Adjust for producer_version
         expected = expected.replace(
-            'producer_version: "CURRENT_VERSION"',
-            'producer_version: "{}"'.format(torch.onnx.producer_version)
+            'producer_version: "CURRENT_VERSION"', 'producer_version: "{}"'.format(torch.onnx.producer_version)
         )
         if expecttest.ACCEPT:
             if expected != s:
@@ -2334,7 +2568,7 @@ class TestCase(expecttest.TestCase):
                 self.assertEqual(s, expected)
 
     def assertExpectedStripMangled(self, s, subname=None):
-        s = re.sub(r'__torch__[^ ]+', '', s)
+        s = re.sub(r"__torch__[^ ]+", "", s)
         self.assertExpected(s, subname)
 
     def assertGreaterAlmostEqual(self, first, second, places=None, msg=None, delta=None):
@@ -2372,11 +2606,7 @@ class TestCase(expecttest.TestCase):
     def run_process_no_exception(code, env=None):
         import subprocess
 
-        popen = subprocess.Popen(
-            [sys.executable, '-c', code],
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE,
-            env=env)
+        popen = subprocess.Popen([sys.executable, "-c", code], stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
         (stdout, stderr) = popen.communicate()
         return (stdout, stderr)
 
@@ -2390,7 +2620,7 @@ class TestCase(expecttest.TestCase):
         if "IN_CI" in env.keys():
             del env["IN_CI"]
         (stdout, stderr) = TestCase.run_process_no_exception(code, env=env)
-        return stderr.decode('ascii')
+        return stderr.decode("ascii")
 
 
 def download_file(url, binary=True):
@@ -2398,14 +2628,14 @@ def download_file(url, binary=True):
     from urllib import request, error
 
     filename = os.path.basename(urlsplit(url)[2])
-    data_dir = get_writable_path(os.path.join(os.path.dirname(__file__), 'data'))
+    data_dir = get_writable_path(os.path.join(os.path.dirname(__file__), "data"))
     path = os.path.join(data_dir, filename)
 
     if os.path.exists(path):
         return path
     try:
         data = request.urlopen(url, timeout=15).read()
-        with open(path, 'wb' if binary else 'w') as f:
+        with open(path, "wb" if binary else "w") as f:
             f.write(data)
         return path
     except error.URLError as e:
@@ -2413,6 +2643,7 @@ def download_file(url, binary=True):
         warnings.warn(msg, RuntimeWarning)
         raise unittest.SkipTest(msg) from e
 
+
 def find_free_port():
     """
     Finds an available port and returns that port number.
@@ -2424,14 +2655,16 @@ def find_free_port():
     """
     with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
         sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-        sock.bind(('localhost', 0))
+        sock.bind(("localhost", 0))
         _, port = sock.getsockname()
         return port
 
+
 # Errors that we can get in c10d initialization for which we should retry tests for.
 ADDRESS_IN_USE = "Address already in use"
 CONNECT_TIMEOUT = "connect() timed out."
 
+
 def retry_on_connect_failures(func=None, connect_errors=(ADDRESS_IN_USE)):
     """Reruns a test if the test returns a RuntimeError and the exception
     contains one of the strings in connect_errors."""
@@ -2454,6 +2687,7 @@ def retry_on_connect_failures(func=None,
                     time.sleep(random.random())
                     continue
                 raise
+
     return wrapper
 
 
@@ -2474,8 +2708,12 @@ def retry(ExceptionToCheck, tries=3, del
             try:
                 return f(*args, **kwargs)
             except ExceptionToCheck as e:
-                raise unittest.SkipTest(f"Skipping after {tries} consecutive {str(e)}") from e if skip_after_retries else e
+                raise unittest.SkipTest(
+                    f"Skipping after {tries} consecutive {str(e)}"
+                ) from e if skip_after_retries else e
+
         return f_retry  # true decorator
+
     return deco_retry
 
 
@@ -2483,7 +2721,8 @@ def retry(ExceptionToCheck, tries=3, del
 #   and review including them in torch.testing
 # Methods for matrix generation
 
-def random_square_matrix_of_rank(l, rank, dtype=torch.double, device='cpu'):
+
+def random_square_matrix_of_rank(l, rank, dtype=torch.double, device="cpu"):
     assert rank <= l
     A = torch.randn(l, l, dtype=dtype, device=device)
     u, s, vh = torch.linalg.svd(A, full_matrices=False)
@@ -2494,6 +2733,7 @@ def random_square_matrix_of_rank(l, rank
             s[i] = 1
     return (u * s.to(dtype).unsqueeze(-2)) @ vh
 
+
 def random_well_conditioned_matrix(*shape, dtype, device, mean=1.0, sigma=0.001):
     """
     Returns a random rectangular matrix (batch of matrices)
@@ -2506,20 +2746,26 @@ def random_well_conditioned_matrix(*shap
         torch.float: torch.float,
         torch.double: torch.double,
         torch.cfloat: torch.float,
-        torch.cdouble: torch.double
+        torch.cdouble: torch.double,
     }
     x = torch.rand(shape, dtype=dtype, device=device)
     m = x.size(-2)
     n = x.size(-1)
     u, _, vh = torch.linalg.svd(x, full_matrices=False)
-    s = (torch.randn(*(shape[:-2] + (min(m, n),)), dtype=primitive_dtype[dtype], device=device) * sigma + mean) \
-        .sort(-1, descending=True).values.to(dtype)
+    s = (
+        (torch.randn(*(shape[:-2] + (min(m, n),)), dtype=primitive_dtype[dtype], device=device) * sigma + mean)
+        .sort(-1, descending=True)
+        .values.to(dtype)
+    )
     return (u * s.unsqueeze(-2)) @ vh
 
+
 # Returns a noncontiguous (tensor with the same shape and values as t
 # The noncontiguous tensor is constructed such that elements in the innermost
 #   dimension are separated by zeros or (whenever possible) nans
 # TODO: consider more complicated noncontiguity schemes
+
+
 def noncontiguous_like(t):
     # Short-circuits if t is already noncontiguous
     if not t.is_contiguous():
@@ -2551,25 +2797,32 @@ def noncontiguous_like(t):
     result.requires_grad_(t.requires_grad)
     return result
 
+
 # TODO: remove this (prefer make_symmetric_matrices below)
+
+
 def random_symmetric_matrix(l, *batches, **kwargs):
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
     A = torch.randn(*(batches + (l, l)), dtype=dtype, device=device)
     A = (A + A.mT).div_(2)
     return A
 
+
 # Creates a symmetric matrix or batch of symmetric matrices
 # Shape must be a square matrix or batch of square matrices
+
+
 def make_symmetric_matrices(*shape, device, dtype):
     assert shape[-1] == shape[-2]
     t = make_tensor(shape, device=device, dtype=dtype)
     t = (t + t.mT).div_(2)
     return t
 
+
 def random_hermitian_matrix(l, *batches, **kwargs):
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
     A = torch.randn(*(batches + (l, l)), dtype=dtype, device=device)
     A = (A + A.mH).div_(2)
     return A
@@ -2582,13 +2835,13 @@ def random_symmetric_psd_matrix(l, *batc
     The following example creates a tensor of size 2 x 4 x 3 x 3
     >>> matrices = random_symmetric_psd_matrix(3, 2, 4, dtype=dtype, device=device)
     """
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
     A = torch.randn(*(batches + (l, l)), dtype=dtype, device=device)
     return A @ A.mT
 
 
-def random_hermitian_psd_matrix(matrix_size, *batch_dims, dtype=torch.double, device='cpu'):
+def random_hermitian_psd_matrix(matrix_size, *batch_dims, dtype=torch.double, device="cpu"):
     """
     Returns a batch of random Hermitian positive-semi-definite matrices.
     The shape of the result is batch_dims + (matrix_size, matrix_size)
@@ -2601,12 +2854,10 @@ def random_hermitian_psd_matrix(matrix_s
 
 # TODO: remove this (prefer make_symmetric_pd_matrices below)
 def random_symmetric_pd_matrix(matrix_size, *batch_dims, **kwargs):
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
-    A = torch.randn(*(batch_dims + (matrix_size, matrix_size)),
-                    dtype=dtype, device=device)
-    return torch.matmul(A, A.mT) \
-        + torch.eye(matrix_size, dtype=dtype, device=device) * 1e-5
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
+    A = torch.randn(*(batch_dims + (matrix_size, matrix_size)), dtype=dtype, device=device)
+    return torch.matmul(A, A.mT) + torch.eye(matrix_size, dtype=dtype, device=device) * 1e-5
 
 
 # Creates a symmetric positive-definite matrix or batch of
@@ -2617,6 +2868,7 @@ def make_symmetric_pd_matrices(*shape, d
     i = torch.eye(shape[-1], device=device, dtype=dtype) * 1e-5
     return t @ t.mT + i
 
+
 def random_hermitian_pd_matrix(matrix_size, *batch_dims, dtype, device):
     """
     Returns a batch of random Hermitian positive-definite matrices.
@@ -2624,12 +2876,14 @@ def random_hermitian_pd_matrix(matrix_si
     The following example creates a tensor of size 2 x 4 x 3 x 3
     >>> matrices = random_hermitian_pd_matrix(3, 2, 4, dtype=dtype, device=device)
     """
-    A = torch.randn(*(batch_dims + (matrix_size, matrix_size)),
-                    dtype=dtype, device=device)
+    A = torch.randn(*(batch_dims + (matrix_size, matrix_size)), dtype=dtype, device=device)
     return A @ A.mH + torch.eye(matrix_size, dtype=dtype, device=device)
 
+
 # Creates a full rank matrix with distinct singular values or
 #   a batch of such matrices
+
+
 def make_fullrank_matrices_with_distinct_singular_values(*shape, device, dtype, requires_grad=False):
     with torch.no_grad():
         t = make_tensor(shape, device=device, dtype=dtype)
@@ -2641,16 +2895,17 @@ def make_fullrank_matrices_with_distinct
         # s = [2, 3, ..., k+1]
         s = torch.arange(2, k + 2, dtype=real_dtype, device=device)
         # s = [2, -3, 4, ..., (-1)^k k+1]
-        s[1::2] *= -1.
+        s[1::2] *= -1.0
         # 1 + 1/s so that the singular values are in the range [2/3, 3/2]
         # This gives a condition number of 9/4, which should be good enough
-        s.reciprocal_().add_(1.)
+        s.reciprocal_().add_(1.0)
         # Note that the singular values need not be ordered in an SVD so
         # we don't need need to sort S
         x = (u * s.to(u.dtype)) @ vh
     x.requires_grad_(requires_grad)
     return x
 
+
 def random_matrix(rows, columns, *batch_dims, **kwargs):
     """Return rectangular matrix or batches of rectangular matrices.
 
@@ -2659,8 +2914,8 @@ def random_matrix(rows, columns, *batch_
       device - the device kind
       singular - when True, the output will be singular
     """
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
     silent = kwargs.get("silent", False)
     singular = kwargs.get("singular", False)
     if silent and not torch._C.has_lapack:
@@ -2699,8 +2954,8 @@ def random_sparse_matrix(rows, columns,
     is specified but higher than min(rows, columns)/(rows * columns)
     for non-singular matrices.
     """
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
     singular = kwargs.get("singular", False)
 
     k = min(rows, columns)
@@ -2712,7 +2967,7 @@ def random_sparse_matrix(rows, columns,
     indices = [row_indices, column_indices]
     values = torch.randn(nonzero_elements, dtype=dtype, device=device)
     # ensure that the diagonal dominates
-    values *= torch.tensor([-float(i - j)**2 for i, j in zip(*indices)], dtype=dtype, device=device).exp()
+    values *= torch.tensor([-float(i - j) ** 2 for i, j in zip(*indices)], dtype=dtype, device=device).exp()
     indices_tensor = torch.tensor(indices)
     A = torch.sparse_coo_tensor(indices_tensor, values, (rows, columns), device=device)
     return A.coalesce()
@@ -2732,12 +2987,11 @@ def random_sparse_pd_matrix(matrix_size,
           A = R^T A R
     """
     import math
-    torch = kwargs.get('torch', globals()['torch'])
-    dtype = kwargs.get('dtype', torch.double)
-    device = kwargs.get('device', 'cpu')
-    data = dict([((i, i), float(i + 1) / matrix_size)
-                 for i in range(matrix_size)])
 
+    torch = kwargs.get("torch", globals()["torch"])
+    dtype = kwargs.get("dtype", torch.double)
+    device = kwargs.get("device", "cpu")
+    data = dict([((i, i), float(i + 1) / matrix_size) for i in range(matrix_size)])
 
     def multiply(data, N, i, j, cs, sn, left=True):
         for k in range(N):
@@ -2774,7 +3028,10 @@ def random_sparse_pd_matrix(matrix_size,
     indices_tensor = torch.tensor([icoords, jcoords])
     return torch.sparse_coo_tensor(indices_tensor, values, (matrix_size, matrix_size), dtype=dtype, device=device)
 
+
 # FIXME: remove this by updating test suites using it
+
+
 def do_test_dtypes(self, dtypes, layout, device):
     for dtype in dtypes:
         if dtype != torch.float16:
@@ -2783,7 +3040,10 @@ def do_test_dtypes(self, dtypes, layout,
             self.assertIs(layout, out.layout)
             self.assertEqual(device, out.device)
 
+
 # FIXME: remove this by updating test suites using it
+
+
 def do_test_empty_full(self, dtypes, layout, device):
     shape = torch.Size([2, 3])
 
@@ -2799,28 +3059,46 @@ def do_test_empty_full(self, dtypes, lay
             self.assertEqual(tensor, fill)
 
     def get_int64_dtype(dtype):
-        module = '.'.join(str(dtype).split('.')[1:-1])
+        module = ".".join(str(dtype).split(".")[1:-1])
         if not module:
             return torch.int64
         return operator.attrgetter(module)(torch).int64
 
     default_dtype = torch.get_default_dtype()
     check_value(torch.empty(shape), default_dtype, torch.strided, -1, None, False)
-    check_value(torch.full(shape, -5.), default_dtype, torch.strided, -1, None, False)
+    check_value(torch.full(shape, -5.0), default_dtype, torch.strided, -1, None, False)
     for dtype in dtypes:
         for rg in {dtype.is_floating_point, False}:
             int64_dtype = get_int64_dtype(dtype)
             v = torch.empty(shape, dtype=dtype, device=device, layout=layout, requires_grad=rg)
             check_value(v, dtype, layout, device, None, rg)
             out = v.new()
-            check_value(torch.empty(shape, out=out, device=device, layout=layout, requires_grad=rg),
-                        dtype, layout, device, None, rg)
+            check_value(
+                torch.empty(shape, out=out, device=device, layout=layout, requires_grad=rg),
+                dtype,
+                layout,
+                device,
+                None,
+                rg,
+            )
             check_value(v.new_empty(shape), dtype, layout, device, None, False)
-            check_value(v.new_empty(shape, dtype=int64_dtype, device=device, requires_grad=False),
-                        int64_dtype, layout, device, None, False)
+            check_value(
+                v.new_empty(shape, dtype=int64_dtype, device=device, requires_grad=False),
+                int64_dtype,
+                layout,
+                device,
+                None,
+                False,
+            )
             check_value(torch.empty_like(v), dtype, layout, device, None, False)
-            check_value(torch.empty_like(v, dtype=int64_dtype, layout=layout, device=device, requires_grad=False),
-                        int64_dtype, layout, device, None, False)
+            check_value(
+                torch.empty_like(v, dtype=int64_dtype, layout=layout, device=device, requires_grad=False),
+                int64_dtype,
+                layout,
+                device,
+                None,
+                False,
+            )
 
             if dtype is not torch.float16 and layout != torch.sparse_coo:
                 fv = 3
@@ -2828,34 +3106,59 @@ def do_test_empty_full(self, dtypes, lay
                 check_value(v, dtype, layout, device, fv, rg)
                 check_value(v.new_full(shape, fv + 1), dtype, layout, device, fv + 1, False)
                 out = v.new()
-                check_value(torch.full(shape, fv + 2, out=out, device=device, layout=layout, requires_grad=rg),
-                            dtype, layout, device, fv + 2, rg)
-                check_value(v.new_full(shape, fv + 3, dtype=int64_dtype, device=device, requires_grad=False),
-                            int64_dtype, layout, device, fv + 3, False)
+                check_value(
+                    torch.full(shape, fv + 2, out=out, device=device, layout=layout, requires_grad=rg),
+                    dtype,
+                    layout,
+                    device,
+                    fv + 2,
+                    rg,
+                )
+                check_value(
+                    v.new_full(shape, fv + 3, dtype=int64_dtype, device=device, requires_grad=False),
+                    int64_dtype,
+                    layout,
+                    device,
+                    fv + 3,
+                    False,
+                )
                 check_value(torch.full_like(v, fv + 4), dtype, layout, device, fv + 4, False)
-                check_value(torch.full_like(v, fv + 5,
-                                            dtype=int64_dtype, layout=layout, device=device, requires_grad=False),
-                            int64_dtype, layout, device, fv + 5, False)
+                check_value(
+                    torch.full_like(v, fv + 5, dtype=int64_dtype, layout=layout, device=device, requires_grad=False),
+                    int64_dtype,
+                    layout,
+                    device,
+                    fv + 5,
+                    False,
+                )
+
 
 # FIXME: improve load_tests() documentation here
 running_script_path = None
+
+
 def set_running_script_path():
     global running_script_path
     try:
         running_file = os.path.abspath(os.path.realpath(sys.argv[0]))
-        if running_file.endswith('.py'):  # skip if the running file is not a script
+        if running_file.endswith(".py"):  # skip if the running file is not a script
             running_script_path = running_file
     except Exception:
         pass
 
+
 def check_test_defined_in_running_script(test_case):
     if running_script_path is None:
         return
     test_case_class_file = os.path.abspath(os.path.realpath(inspect.getfile(test_case.__class__)))
-    assert test_case_class_file == running_script_path, "Class of loaded TestCase \"{}\" " \
-        "is not defined in the running script \"{}\", but in \"{}\". Did you " \
+    assert test_case_class_file == running_script_path, (
+        'Class of loaded TestCase "{}" '
+        'is not defined in the running script "{}", but in "{}". Did you '
         "accidentally import a unittest.TestCase from another file?".format(
-            test_case.id(), running_script_path, test_case_class_file)
+            test_case.id(), running_script_path, test_case_class_file
+        )
+    )
+
 
 def load_tests(loader, tests, pattern):
     set_running_script_path()
@@ -2875,6 +3178,7 @@ class BytesIOContext(io.BytesIO):
     def __exit__(self, *args):
         pass
 
+
 # Tentative value for nondet_tol for gradcheck when backward implementation
 # relies on nondeterministic operations, i.e., those listed here:
 # https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
@@ -2882,6 +3186,7 @@ class BytesIOContext(io.BytesIO):
 # For more information see https://github.com/pytorch/pytorch/issues/56202
 GRADCHECK_NONDET_TOL = 1e-12
 
+
 def gradcheck(fn, inputs, **kwargs):
     # Wrapper around gradcheck that enables certain keys by default.
     # Use this testing-internal gradcheck instead of autograd.gradcheck so that new features like vmap and
@@ -2894,7 +3199,7 @@ def gradcheck(fn, inputs, **kwargs):
         "fast_mode": True,
     }
 
-    if os.environ.get('PYTORCH_TEST_WITH_SLOW_GRADCHECK', "0FF") == "ON":
+    if os.environ.get("PYTORCH_TEST_WITH_SLOW_GRADCHECK", "0FF") == "ON":
         default_values["fast_mode"] = False
 
     for key, value in default_values.items():
@@ -2904,6 +3209,7 @@ def gradcheck(fn, inputs, **kwargs):
 
     return torch.autograd.gradcheck(fn, inputs, **kwargs)
 
+
 def gradgradcheck(fn, inputs, grad_outputs=None, **kwargs):
     # Wrapper around gradgradcheck that enables certain keys by default
     # See gradcheck above for an explanation of why we need something like this.
@@ -2914,7 +3220,7 @@ def gradgradcheck(fn, inputs, grad_outpu
         "fast_mode": True,
     }
 
-    if os.environ.get('PYTORCH_TEST_WITH_SLOW_GRADCHECK', "0FF") == "ON":
+    if os.environ.get("PYTORCH_TEST_WITH_SLOW_GRADCHECK", "0FF") == "ON":
         default_values["fast_mode"] = False
 
     for key, value in default_values.items():
@@ -2945,19 +3251,19 @@ def set_cwd(path: str) -> Iterator[None]
 # FIXME: delete this
 # Using @toleranceOverride specific to your test is the recommended way
 # of doing this. These are just some values that worked for test_nn.
-dtype2prec_DONTUSE = {torch.float: 1e-5,
-                      torch.double: 1e-5,
-                      torch.half: 1e-2,
-                      torch.bfloat16: 1e-1}
+dtype2prec_DONTUSE = {torch.float: 1e-5, torch.double: 1e-5, torch.half: 1e-2, torch.bfloat16: 1e-1}
 
 # FIXME: move to test_sparse or sparse utils
 # This is a wrapper that wraps a test to run this test twice, one with
 # coalesced=True, another with coalesced=False for coalesced/uncoalesced sparse tensors.
+
+
 def coalescedonoff(f):
     @wraps(f)
     def wrapped(self, *args, **kwargs):
         f(self, *args, **kwargs, coalesced=True)
         f(self, *args, **kwargs, coalesced=False)
+
     return wrapped
 
 
@@ -2977,11 +3283,12 @@ def find_library_location(lib_name: str)
     # return the shared library file in the installed folder if exist,
     # else the file in the build folder
     torch_root = Path(torch.__file__).resolve().parent
-    path = torch_root / 'lib' / lib_name
+    path = torch_root / "lib" / lib_name
     if os.path.exists(path):
         return path
     torch_root = Path(__file__).resolve().parent.parent.parent
-    return torch_root / 'build' / 'lib' / lib_name
+    return torch_root / "build" / "lib" / lib_name
+
 
 def sandcastle_skip(reason):
     """
@@ -2989,6 +3296,7 @@ def sandcastle_skip(reason):
     "passes" the test instead to avoid creating tasks complaining about tests
     skipping continuously.
     """
+
     def decorator(func):
         if not IS_SANDCASTLE:
             func.__unittest_skip__ = True
@@ -2997,12 +3305,14 @@ def sandcastle_skip(reason):
 
         @wraps(func)
         def wrapper(*args, **kwargs):
-            print(f'Skipping {func.__name__} on sandcastle for following reason: {reason}', file=sys.stderr)
+            print(f"Skipping {func.__name__} on sandcastle for following reason: {reason}", file=sys.stderr)
             return
+
         return wrapper
 
     return decorator
 
+
 def mock_wrapper(method):
     """
     Returns a function that calls the real implementation of a method
@@ -3014,13 +3324,14 @@ def mock_wrapper(method):
     def wrapper(self, *args, **kwargs):
         mock(*args, **kwargs)
         return method(self, *args, **kwargs)
+
     wrapper.mock = mock  # type: ignore[attr-defined]
     return wrapper
 
+
 def get_tensors_from(args, kwargs):
     """ Returns a set of all Tensor objects in the given args and kwargs. """
-    return set([arg for arg in args if isinstance(arg, Tensor)] +
-               [v for v in kwargs.values() if isinstance(v, Tensor)])
+    return set([arg for arg in args if isinstance(arg, Tensor)] + [v for v in kwargs.values() if isinstance(v, Tensor)])
 
 
 # Returns scalar tensor representation of a list of integer byte values
@@ -3047,16 +3358,13 @@ def bytes_to_scalar(byte_list: List[int]
     if dtype.is_complex:
         assert len(byte_list) == (num_bytes * 2)
         check_bytes(byte_list)
-        real = ctype.from_buffer((ctypes.c_byte * num_bytes)(
-            *byte_list[:num_bytes])).value
-        imag = ctype.from_buffer((ctypes.c_byte * num_bytes)(
-            *byte_list[num_bytes:])).value
+        real = ctype.from_buffer((ctypes.c_byte * num_bytes)(*byte_list[:num_bytes])).value
+        imag = ctype.from_buffer((ctypes.c_byte * num_bytes)(*byte_list[num_bytes:])).value
         res = real + 1j * imag
     else:
         assert len(byte_list) == num_bytes
         check_bytes(byte_list)
-        res = ctype.from_buffer((ctypes.c_byte * num_bytes)(
-            *byte_list)).value
+        res = ctype.from_buffer((ctypes.c_byte * num_bytes)(*byte_list)).value
 
     return torch.tensor(res, device=device, dtype=dtype)
 
@@ -3082,6 +3390,7 @@ def sandcastle_skip_if(condition, reason
     "passes" the test instead to avoid creating tasks complaining about tests
     skipping continuously.
     """
+
     def decorator(func):
 
         if not IS_SANDCASTLE and condition:
@@ -3092,17 +3401,19 @@ def sandcastle_skip_if(condition, reason
         @wraps(func)
         def wrapper(*args, **kwargs):
             if condition and IS_SANDCASTLE:
-                print(f'Skipping {func.__name__} on sandcastle for following reason: {reason}', file=sys.stderr)
+                print(f"Skipping {func.__name__} on sandcastle for following reason: {reason}", file=sys.stderr)
                 return
             else:
                 return func(*args, **kwargs)
+
         return wrapper
 
     return decorator
 
+
 def dtype_name(dtype):
     """ Returns the pretty name of the dtype (e.g. torch.int64 -> int64). """
-    return str(dtype).split('.')[1]
+    return str(dtype).split(".")[1]
 
 
 def set_single_threaded_if_parallel_tbb(fn):
@@ -3121,6 +3432,7 @@ def set_single_threaded_if_parallel_tbb(
             return fn(*args, **kwargs)
         finally:
             torch.set_num_threads(num_threads)
+
     return wrap_fn
 
 
@@ -3152,12 +3464,14 @@ def get_cycles_per_ms() -> float:
     for _ in range(num):
         vals.append(measure())
     vals = sorted(vals)
-    return mean(vals[2 : num - 2])
+    return mean(vals[2: num - 2])
 
 
 # OpInfo utils
 
-T = TypeVar('T')
+T = TypeVar("T")
+
+
 def first_sample(self: unittest.TestCase, samples: Iterable[T]) -> T:
     """
     Returns the first sample from an iterable of samples, like those returned by OpInfo.
@@ -3166,10 +3480,13 @@ def first_sample(self: unittest.TestCase
     try:
         return next(iter(samples))
     except StopIteration:
-        raise unittest.SkipTest('Skipped! Need at least 1 sample input')
+        raise unittest.SkipTest("Skipped! Need at least 1 sample input")
+
 
 # this helper method is to recursively
 # clone the tensor-type input of operators tested by OpInfo
+
+
 def clone_input_helper(input):
     if isinstance(input, torch.Tensor):
         return torch.clone(input)
